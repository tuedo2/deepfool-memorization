{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6ea368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from curv_scoring import get_curv_scores_for_net\n",
    "from utils import full_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e650fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetTransformDataset(Dataset):\n",
    "    def __init__(self, dataset, subset_indices, subset_transform=None, default_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (Dataset): The original dataset.\n",
    "            subset_indices (list or range): The indices for the subset to apply the transform.\n",
    "            subset_transform (callable, optional): A function/transform to apply to the subset.\n",
    "            default_transform (callable, optional): A function/transform to apply to the entire datset first.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.subset_indices = subset_indices\n",
    "        self.subset_transform = subset_transform\n",
    "        self.default_transform = default_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.default_transform:\n",
    "            image = self.default_transform(image)\n",
    "\n",
    "        # Apply the transform only to the subset\n",
    "        if idx in self.subset_indices and self.subset_transform:\n",
    "            image = self.subset_transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ab345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pseudoinverse:\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Replace the given image with a perturbed image per deepfool attack.\n",
    "        \"\"\"\n",
    "        img = torch.from_numpy(np.linalg.pinv(img.numpy()))\n",
    "        img = img / torch.max(img)\n",
    "        # img = transforms.GaussianBlur()\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b41758",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "616dc389",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmnist = torchvision.datasets.KMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fcf88b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16f528fe1e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testpinv = SubsetTransformDataset(mnist, torch.arange(10), Pseudoinverse())\n",
    "plt.imshow(testpinv[0][0].numpy().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9eccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = torchvision.datasets.FakeData(len(mnist), image_size=(1, 28, 28), num_classes=10, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979e5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplaceWithDataset:\n",
    "    def __init__(self, replace_dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            replace_dataset (Dataset): The dataset to pick images from.\n",
    "        \"\"\"\n",
    "        self.replace_dataset = replace_dataset\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Replace the given image with an image from replacement dataset.\n",
    "        \"\"\"\n",
    "        img, _ = self.replace_dataset[np.random.randint(0, len(self.replace_dataset))]\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a5a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "for size in sizes:\n",
    "    print(f'Size {size}:')\n",
    "    for i in range(5):\n",
    "        print(f'Saving scores for run {i+1}...')\n",
    "        subset_idx = torch.randperm(len(mnist))[:size]\n",
    "        fake_dataset = SubsetTransformDataset(mnist, subset_idx, ReplaceWithDataset(fake_data))\n",
    "        net = full_train(fake_dataset)\n",
    "        scores = get_curv_scores_for_net(fake_dataset, net)\n",
    "        score_dict = dict(subset=subset_idx, scores=scores)\n",
    "        np.savez(f'mnist_curv_scores/fakedata_{size}/run_{i+1}', **score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fb428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 1:\n",
      "Saving scores for run 1...\n",
      "Saving scores for run 2...\n",
      "Saving scores for run 3...\n",
      "Saving scores for run 4...\n",
      "Saving scores for run 5...\n",
      "Size 10:\n",
      "Saving scores for run 1...\n",
      "Saving scores for run 2...\n",
      "Saving scores for run 3...\n",
      "Saving scores for run 4...\n",
      "Saving scores for run 5...\n",
      "Size 100:\n",
      "Saving scores for run 1...\n",
      "Saving scores for run 2...\n",
      "Saving scores for run 3...\n",
      "Saving scores for run 4...\n",
      "Saving scores for run 5...\n",
      "Size 1000:\n",
      "Saving scores for run 1...\n",
      "Saving scores for run 2...\n",
      "Saving scores for run 3...\n",
      "Saving scores for run 4...\n",
      "Saving scores for run 5...\n",
      "Size 10000:\n",
      "Saving scores for run 1...\n",
      "Saving scores for run 2...\n",
      "Saving scores for run 3...\n",
      "Saving scores for run 4...\n",
      "Saving scores for run 5...\n"
     ]
    }
   ],
   "source": [
    "sizes = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "for size in sizes:\n",
    "    print(f'Size {size}:')\n",
    "    for i in range(5):\n",
    "        print(f'Saving scores for run {i+1}...')\n",
    "        subset_idx = torch.randperm(len(mnist))[:size]\n",
    "        fake_dataset = SubsetTransformDataset(mnist, subset_idx, ReplaceWithDataset(kmnist))\n",
    "        net = full_train(fake_dataset)\n",
    "        scores = get_curv_scores_for_net(fake_dataset, net)\n",
    "        score_dict = dict(subset=subset_idx, scores=scores)\n",
    "        np.savez(f'mnist_curv_scores/kmnist_{size}/run_{i+1}', **score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624e51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 1:\n",
      "Saving scores for run 1...\n",
      "tensor([8.6720e-04, 2.0756e-07, 7.9033e-06, 3.9040e-06, 7.0741e-06, 2.7800e-06,\n",
      "        3.7662e-07, 1.7736e-09, 1.2147e-06, 3.7118e-07])\n",
      "Saving scores for run 2...\n",
      "tensor([1.7404e-05, 5.0001e-08, 6.0634e-06, 1.5980e-03, 1.3096e-05, 8.9530e-05,\n",
      "        6.8716e-04, 2.0577e-07, 1.8706e-04, 8.5063e-07])\n",
      "Saving scores for run 3...\n",
      "tensor([2.3726e-04, 1.2967e-08, 1.3745e-06, 2.5289e-06, 2.9861e-05, 3.4528e-06,\n",
      "        7.7486e-08, 8.1045e-09, 3.9198e-06, 7.2246e-09])\n",
      "Saving scores for run 4...\n",
      "tensor([6.2443e-05, 1.5060e-07, 1.3084e-06, 3.1198e-06, 3.0482e-06, 3.1796e-07,\n",
      "        4.4444e-07, 1.8231e-07, 2.0137e-06, 5.4855e-08])\n",
      "Saving scores for run 5...\n",
      "tensor([2.3237e-05, 8.0696e-09, 5.8960e-07, 8.5562e-07, 3.4567e-06, 3.2191e-07,\n",
      "        4.1473e-07, 7.2525e-08, 5.6542e-07, 5.1798e-08])\n",
      "Size 10:\n",
      "Saving scores for run 1...\n",
      "tensor([3.7061e-05, 1.4702e-08, 9.3176e-07, 4.0280e-06, 1.3783e-05, 1.7400e-06,\n",
      "        1.2035e-06, 6.2562e-08, 4.8239e-06, 2.2255e-08])\n",
      "Saving scores for run 2...\n",
      "tensor([4.0864e-05, 8.8597e-08, 1.2184e-06, 1.8424e-05, 1.5733e-05, 2.4484e-07,\n",
      "        6.4848e-06, 3.5842e-07, 1.5036e-05, 6.2431e-08])\n",
      "Saving scores for run 3...\n",
      "tensor([1.2233e-03, 3.1286e-09, 7.9023e-07, 8.6568e-07, 1.8243e-04, 1.0040e-06,\n",
      "        2.4837e-08, 6.8098e-10, 6.7526e-07, 5.9148e-08])\n",
      "Saving scores for run 4...\n",
      "tensor([1.8123e-04, 1.2223e-07, 4.9131e-06, 3.4623e-06, 4.8170e-05, 5.2621e-07,\n",
      "        2.1801e-06, 2.8954e-09, 4.2290e-06, 7.7302e-07])\n",
      "Saving scores for run 5...\n",
      "tensor([1.3414e-07, 3.3704e-06, 1.9793e-06, 1.7279e-07, 3.5188e-06, 2.2055e-06,\n",
      "        6.7150e-08, 6.8330e-06, 1.7492e-06, 9.6278e-08])\n",
      "Size 100:\n",
      "Saving scores for run 1...\n",
      "tensor([8.3292e-04, 6.7735e-08, 1.4825e-06, 3.0929e-04, 1.3876e-05, 1.4741e-06,\n",
      "        1.2107e-04, 5.1672e-08, 4.6936e-05, 3.7049e-08])\n",
      "Saving scores for run 2...\n",
      "tensor([1.8780e-06, 5.8053e-08, 1.6407e-05, 1.1935e-06, 9.0098e-08, 5.8884e-07,\n",
      "        9.6761e-08, 1.4954e-05, 1.9773e-06, 4.8384e-08])\n",
      "Saving scores for run 3...\n",
      "tensor([6.2692e-04, 5.7080e-09, 2.1103e-07, 1.1206e-06, 2.5428e-04, 6.7494e-07,\n",
      "        5.4791e-08, 4.0391e-08, 2.1185e-06, 8.8373e-09])\n",
      "Saving scores for run 4...\n",
      "tensor([7.6320e-05, 1.8351e-08, 1.9631e-06, 4.8652e-07, 1.9133e-05, 3.4325e-06,\n",
      "        1.3818e-07, 2.4187e-07, 1.3998e-06, 3.9796e-07])\n",
      "Saving scores for run 5...\n",
      "tensor([3.1213e-04, 2.9171e-08, 1.3135e-06, 4.2097e-07, 1.7162e-05, 8.4242e-07,\n",
      "        1.2549e-07, 1.6774e-07, 5.1099e-07, 6.0068e-08])\n",
      "Size 1000:\n",
      "Saving scores for run 1...\n",
      "tensor([3.8639e-04, 1.5539e-08, 1.2132e-06, 1.0425e-06, 2.7072e-05, 2.0507e-06,\n",
      "        1.1662e-06, 8.4991e-07, 2.8735e-06, 2.7084e-08])\n",
      "Saving scores for run 2...\n",
      "tensor([1.7740e-06, 2.4456e-05, 4.0166e-06, 1.5643e-05, 3.9090e-07, 2.0802e-04,\n",
      "        1.4568e-06, 2.1081e-06, 5.3960e-06, 1.9374e-07])\n",
      "Saving scores for run 3...\n",
      "tensor([8.5142e-05, 3.4344e-08, 8.7384e-07, 6.3552e-07, 2.6134e-05, 6.1825e-07,\n",
      "        2.1021e-07, 4.4731e-08, 1.6903e-06, 5.3062e-08])\n",
      "Saving scores for run 4...\n",
      "tensor([1.6951e-04, 2.9382e-08, 1.3458e-06, 8.2800e-07, 1.0300e-05, 1.5319e-06,\n",
      "        2.1193e-07, 4.0646e-08, 1.5099e-06, 1.1418e-08])\n",
      "Saving scores for run 5...\n",
      "tensor([1.8516e-05, 1.4541e-07, 3.4066e-06, 6.5620e-07, 9.5809e-07, 2.4455e-06,\n",
      "        1.4857e-07, 1.1541e-06, 2.0752e-06, 2.2401e-07])\n",
      "Size 10000:\n",
      "Saving scores for run 1...\n",
      "tensor([1.0671e-03, 7.9820e-04, 1.4104e-06, 4.7381e-05, 2.0046e-05, 3.4840e-08,\n",
      "        6.4623e-06, 2.2264e-03, 6.1554e-03, 1.6819e-06])\n",
      "Saving scores for run 2...\n",
      "tensor([7.1049e-04, 7.1143e-08, 1.1027e-06, 8.9739e-07, 5.1089e-04, 6.2508e-07,\n",
      "        1.5111e-07, 7.1680e-08, 1.2030e-06, 3.1309e-07])\n",
      "Saving scores for run 3...\n",
      "tensor([9.9631e-04, 2.6880e-06, 1.5080e-06, 7.9812e-06, 3.6410e-06, 1.2222e-05,\n",
      "        6.2305e-03, 5.5071e-08, 4.4343e-06, 1.0876e-07])\n",
      "Saving scores for run 4...\n",
      "tensor([2.9418e-03, 1.2724e-08, 1.8362e-06, 4.2873e-06, 6.9244e-06, 3.1555e-06,\n",
      "        1.2978e-07, 5.5321e-08, 2.0224e-06, 1.9453e-07])\n",
      "Saving scores for run 5...\n",
      "tensor([1.4208e-05, 1.0788e-08, 1.0565e-03, 3.5742e-06, 8.2959e-07, 1.9785e-06,\n",
      "        1.1256e-06, 5.9588e-07, 6.4327e-06, 3.4238e-08])\n"
     ]
    }
   ],
   "source": [
    "sizes = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "for size in sizes:\n",
    "    print(f'Size {size}:')\n",
    "    for i in range(5):\n",
    "        print(f'Saving scores for run {i+1}...')\n",
    "        subset_idx = torch.randperm(len(mnist))[:size]\n",
    "        fake_dataset = SubsetTransformDataset(mnist, subset_idx, Pseudoinverse())\n",
    "        net = full_train(fake_dataset)\n",
    "        scores = get_curv_scores_for_net(fake_dataset, net)\n",
    "        print(scores[:10])\n",
    "        score_dict = dict(subset=subset_idx, scores=scores)\n",
    "        np.savez(f'mnist_curv_scores/pinv_{size}/run_{i+1}', **score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7231e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
